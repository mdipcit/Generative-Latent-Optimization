# FIDæœ€é©åŒ–å®Ÿé¨“ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆè¨­è¨ˆ

## ğŸ¯ ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ§‹æˆ

### å®Ÿè¡Œæ®µéšåˆ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
1. **pilot_experiment.py**: å°è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆ30æšï¼‰
2. **medium_scale_experiment.py**: ä¸­è¦æ¨¡å®Ÿé¨“ï¼ˆ150æšï¼‰
3. **full_scale_experiment.py**: å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“ï¼ˆ500æšï¼‰
4. **analyze_results.py**: çµæœåˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

## ğŸ“ æ¨å¥¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
experiments/
â”œâ”€â”€ fid_comparison/
â”‚   â”œâ”€â”€ pilot_experiment.py
â”‚   â”œâ”€â”€ medium_scale_experiment.py
â”‚   â”œâ”€â”€ full_scale_experiment.py
â”‚   â”œâ”€â”€ analyze_results.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ experiment_utils.py
â”‚   â”‚   â””â”€â”€ visualization_utils.py
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ pilot/
â”‚       â”œâ”€â”€ medium/
â”‚       â”œâ”€â”€ full/
â”‚       â””â”€â”€ analysis/
```

## ğŸš€ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆè©³ç´°è¨­è¨ˆ

### 1. experiments/fid_comparison/pilot_experiment.py
```python
#!/usr/bin/env python3
"""
FIDæœ€é©åŒ–å®Ÿé¨“: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆç‰ˆ
å°è¦æ¨¡ãƒ†ã‚¹ãƒˆã§ã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèªã¨å‡¦ç†æ™‚é–“æ¨å®š
"""

import os
import sys
import time
import json
from pathlib import Path
import logging
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

from generative_latent_optimization import OptimizationConfig
from generative_latent_optimization.workflows import process_bsds500_dataset
from generative_latent_optimization.metrics import DatasetFIDEvaluator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PilotExperiment:
    """ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“åˆ¶å¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_images_per_split=10):
        self.max_images = max_images_per_split
        self.output_base = Path(__file__).parent / "results" / "pilot"
        self.bsds500_path = os.environ.get('BSDS500_PATH')
        
        # ç¾åœ¨å®Ÿè£…æ¸ˆã¿ã®æŒ‡æ¨™ã®ã¿ãƒ†ã‚¹ãƒˆ
        self.test_metrics = ['mse', 'l1']
        
        # å®Ÿé¨“ç’°å¢ƒæ¤œè¨¼
        self._validate_environment()
    
    def _validate_environment(self):
        """å®Ÿé¨“ç’°å¢ƒã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if not self.bsds500_path:
            raise ValueError("BSDS500_PATH environment variable not set")
        
        if not Path(self.bsds500_path).exists():
            raise FileNotFoundError(f"BSDS500 dataset not found: {self.bsds500_path}")
        
        # HF_TOKENç¢ºèª
        if not os.environ.get('HF_TOKEN'):
            logger.warning("HF_TOKEN not set - VAE model loading may fail")
    
    def run_experiment(self):
        """ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“å®Ÿè¡Œ"""
        
        logger.info("ğŸš€ Starting FID Pilot Experiment")
        logger.info(f"   Max images per split: {self.max_images}")
        logger.info(f"   Output directory: {self.output_base}")
        logger.info(f"   Test metrics: {self.test_metrics}")
        
        experiment_results = {}
        total_start_time = time.time()
        
        for metric in self.test_metrics:
            logger.info(f"\nğŸ”„ Testing {metric} optimization")
            metric_start_time = time.time()
            
            try:
                result = self._run_single_metric_experiment(metric)
                experiment_results[metric] = result
                
                metric_duration = time.time() - metric_start_time
                logger.info(f"âœ… {metric} completed: FID = {result['fid_score']:.2f} ({metric_duration/60:.1f}min)")
                
            except Exception as e:
                logger.error(f"âŒ {metric} experiment failed: {e}")
                experiment_results[metric] = {'error': str(e)}
        
        total_duration = time.time() - total_start_time
        
        # çµæœä¿å­˜
        results_summary = {
            'experiment_type': 'pilot',
            'max_images_per_split': self.max_images,
            'total_duration_minutes': total_duration / 60,
            'timestamp': datetime.now().isoformat(),
            'results': experiment_results
        }
        
        self._save_results(results_summary)
        self._print_summary(experiment_results, total_duration)
        
        return experiment_results
    
    def _run_single_metric_experiment(self, metric):
        """å˜ä¸€æŒ‡æ¨™ã§ã®å®Ÿé¨“å®Ÿè¡Œ"""
        
        # æœ€é©åŒ–è¨­å®š
        config = OptimizationConfig(
            iterations=50,  # ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã§ã¯çŸ­ç¸®
            learning_rate=0.4,
            loss_function=metric,
            device='cuda'
        )
        
        # å‡ºåŠ›ãƒ‘ã‚¹
        metric_output_dir = self.output_base / metric
        metric_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€é©åŒ–
        start_time = time.time()
        datasets = process_bsds500_dataset(
            self.bsds500_path,
            metric_output_dir / "dataset",
            config,
            max_images_per_split=self.max_images,
            create_pytorch_dataset=False,  # ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã§ã¯è»½é‡åŒ–
            create_png_dataset=True
        )
        processing_time = time.time() - start_time
        
        # FIDè©•ä¾¡
        fid_evaluator = DatasetFIDEvaluator(device='cuda')
        fid_result = fid_evaluator.evaluate_created_dataset_vs_original(
            datasets['png'], 
            self.bsds500_path
        )
        
        return {
            'fid_score': fid_result.fid_score,
            'processing_time_seconds': processing_time,
            'total_images': fid_result.total_images,
            'dataset_path': str(datasets['png']),
            'optimization_config': {
                'iterations': config.iterations,
                'learning_rate': config.learning_rate,
                'loss_function': config.loss_function
            }
        }
    
    def _save_results(self, results):
        """å®Ÿé¨“çµæœã®ä¿å­˜"""
        output_file = self.output_base / "pilot_results.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Results saved to: {output_file}")
    
    def _print_summary(self, results, total_duration):
        """çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š FID Pilot Experiment Results")
        print("="*60)
        
        for metric, result in results.items():
            if 'error' in result:
                print(f"âŒ {metric.upper()}: FAILED - {result['error']}")
            else:
                fid = result['fid_score']
                time_min = result['processing_time_seconds'] / 60
                print(f"âœ… {metric.upper()}: FID = {fid:.2f} ({time_min:.1f}min)")
        
        print(f"\nâ±ï¸  Total experiment time: {total_duration/60:.1f} minutes")
        print("="*60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    experiment = PilotExperiment(max_images_per_split=10)
    results = experiment.run_experiment()
    
    # æˆåŠŸåˆ¤å®š
    successful_count = len([r for r in results.values() if 'error' not in r])
    total_count = len(results)
    
    print(f"\nğŸ¯ Experiment completed: {successful_count}/{total_count} metrics tested successfully")
    
    if successful_count == total_count:
        print("âœ… Ready for medium-scale experiment")
    else:
        print("âš ï¸ Some metrics failed - check logs and fix issues before proceeding")

if __name__ == "__main__":
    main()
```

### 2. experiments/fid_comparison/utils/experiment_utils.py
```python
#!/usr/bin/env python3
"""FIDå®Ÿé¨“ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°"""

import os
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

import torch

logger = logging.getLogger(__name__)

class ExperimentValidator:
    """å®Ÿé¨“ç’°å¢ƒãƒ»è¨­å®šã®æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def validate_environment():
        """å®Ÿé¨“å®Ÿè¡Œç’°å¢ƒã®æ¤œè¨¼"""
        issues = []
        
        # BSDS500ãƒ‘ã‚¹ç¢ºèª
        bsds500_path = os.environ.get('BSDS500_PATH')
        if not bsds500_path:
            issues.append("BSDS500_PATH environment variable not set")
        elif not Path(bsds500_path).exists():
            issues.append(f"BSDS500 dataset not found: {bsds500_path}")
        
        # HF_TOKENç¢ºèª
        if not os.environ.get('HF_TOKEN'):
            issues.append("HF_TOKEN environment variable not set")
        
        # GPUç¢ºèª
        if not torch.cuda.is_available():
            issues.append("CUDA not available - experiments will run slowly on CPU")
        
        # å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèª
        try:
            import lpips
        except ImportError:
            issues.append("lpips package not available - LPIPS experiments will fail")
        
        try:
            from torchmetrics.image import StructuralSimilarityIndexMeasure
        except ImportError:
            issues.append("torchmetrics package not available - SSIM experiments will fail")
        
        return issues
    
    @staticmethod
    def estimate_experiment_time(max_images_per_split, metrics_count):
        """å®Ÿé¨“æ™‚é–“ã®æ¨å®š"""
        
        # åŸºæœ¬å‡¦ç†æ™‚é–“ï¼ˆåˆ†/ç”»åƒ/æŒ‡æ¨™ï¼‰
        time_per_image_per_metric = {
            'mse': 0.5,
            'l1': 0.5,
            'lpips': 1.2,
            'ssim': 0.8,
            'improved_ssim': 0.8,
            'psnr': 0.6
        }
        
        # ç·ç”»åƒæ•°ï¼ˆtrain + val + testï¼‰
        total_images = max_images_per_split * 3 if max_images_per_split else 500
        
        # æ¨å®šæ™‚é–“è¨ˆç®—
        estimated_minutes = total_images * metrics_count * 0.8  # å¹³å‡å‡¦ç†æ™‚é–“
        
        return {
            'total_images': total_images,
            'estimated_minutes': estimated_minutes,
            'estimated_hours': estimated_minutes / 60
        }

class ExperimentLogger:
    """å®Ÿé¨“å°‚ç”¨ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, experiment_name, output_dir):
        self.experiment_name = experiment_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        log_file = self.output_dir / f"{experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # ãƒ­ã‚¬ãƒ¼è¨­å®š
        self.logger = logging.getLogger(experiment_name)
        handler = logging.FileHandler(log_file)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)
    
    def log_experiment_start(self, config):
        """å®Ÿé¨“é–‹å§‹ãƒ­ã‚°"""
        self.logger.info(f"Experiment {self.experiment_name} started")
        self.logger.info(f"Configuration: {config}")
    
    def log_metric_start(self, metric, config):
        """æŒ‡æ¨™åˆ¥å®Ÿé¨“é–‹å§‹ãƒ­ã‚°"""
        self.logger.info(f"Starting {metric} optimization")
        self.logger.info(f"  Iterations: {config.iterations}")
        self.logger.info(f"  Learning rate: {config.learning_rate}")
    
    def log_metric_complete(self, metric, fid_score, processing_time):
        """æŒ‡æ¨™åˆ¥å®Ÿé¨“å®Œäº†ãƒ­ã‚°"""
        self.logger.info(f"{metric} optimization completed")
        self.logger.info(f"  FID score: {fid_score:.2f}")
        self.logger.info(f"  Processing time: {processing_time/60:.1f} minutes")
    
    def log_experiment_complete(self, total_time, results_summary):
        """å®Ÿé¨“å®Œäº†ãƒ­ã‚°"""
        self.logger.info(f"Experiment {self.experiment_name} completed")
        self.logger.info(f"Total time: {total_time/3600:.1f} hours")
        self.logger.info(f"Results summary: {results_summary}")

class ResultsManager:
    """å®Ÿé¨“çµæœç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def save_experiment_results(self, experiment_name, results):
        """å®Ÿé¨“çµæœã®ä¿å­˜"""
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = self.output_dir / f"{experiment_name}_results_{timestamp}.json"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        full_results = {
            'experiment_name': experiment_name,
            'timestamp': datetime.now().isoformat(),
            'environment': {
                'cuda_available': torch.cuda.is_available(),
                'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
                'python_version': sys.version,
                'torch_version': torch.__version__
            },
            'results': results
        }
        
        # JSONä¿å­˜
        with open(results_file, 'w') as f:
            json.dump(full_results, f, indent=2)
        
        logger.info(f"Results saved to: {results_file}")
        return str(results_file)
    
    def load_previous_results(self, experiment_name):
        """éå»ã®å®Ÿé¨“çµæœèª­ã¿è¾¼ã¿"""
        
        pattern = f"{experiment_name}_results_*.json"
        result_files = list(self.output_dir.glob(pattern))
        
        if not result_files:
            return None
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        latest_file = max(result_files, key=os.path.getctime)
        
        with open(latest_file, 'r') as f:
            return json.load(f)

class ExperimentRunner:
    """å®Ÿé¨“å®Ÿè¡Œã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, experiment_name, output_dir, max_images_per_split=None):
        self.experiment_name = experiment_name
        self.output_dir = Path(output_dir)
        self.max_images = max_images_per_split
        
        # ç’°å¢ƒè¨­å®š
        self.bsds500_path = os.environ.get('BSDS500_PATH')
        
        # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£åˆæœŸåŒ–
        self.logger = ExperimentLogger(experiment_name, self.output_dir / "logs")
        self.results_manager = ResultsManager(self.output_dir / "results")
        self.fid_evaluator = DatasetFIDEvaluator(device='cuda')
    
    def run_metric_experiment(self, metric, config):
        """å˜ä¸€æŒ‡æ¨™ã§ã®å®Ÿé¨“å®Ÿè¡Œ"""
        
        self.logger.log_metric_start(metric, config)
        metric_start_time = time.time()
        
        try:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            metric_output_dir = self.output_dir / "datasets" / metric
            metric_output_dir.mkdir(parents=True, exist_ok=True)
            
            # æœ€é©åŒ–å®Ÿè¡Œ
            datasets = process_bsds500_dataset(
                self.bsds500_path,
                metric_output_dir / "optimized",
                config,
                max_images_per_split=self.max_images,
                create_pytorch_dataset=True,
                create_png_dataset=True
            )
            
            # FIDè©•ä¾¡
            fid_result = self.fid_evaluator.evaluate_created_dataset_vs_original(
                datasets['png'], 
                self.bsds500_path
            )
            
            processing_time = time.time() - metric_start_time
            
            # çµæœæ§‹é€ åŒ–
            result = {
                'fid_score': fid_result.fid_score,
                'processing_time_seconds': processing_time,
                'total_images': fid_result.total_images,
                'dataset_paths': datasets,
                'optimization_config': {
                    'iterations': config.iterations,
                    'learning_rate': config.learning_rate,
                    'loss_function': config.loss_function,
                    'device': config.device
                }
            }
            
            self.logger.log_metric_complete(metric, fid_result.fid_score, processing_time)
            return result
            
        except Exception as e:
            self.logger.logger.error(f"{metric} experiment failed: {e}")
            raise

def main():
    """ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ç’°å¢ƒæ¤œè¨¼
    issues = ExperimentValidator.validate_environment()
    if issues:
        print("âš ï¸ Environment issues detected:")
        for issue in issues:
            print(f"   - {issue}")
        print("\nPlease resolve these issues before running the experiment")
        return
    
    # å‡¦ç†æ™‚é–“æ¨å®š
    time_estimate = ExperimentValidator.estimate_experiment_time(
        max_images_per_split=10, 
        metrics_count=2
    )
    
    print(f"ğŸ“Š Experiment Estimate:")
    print(f"   Total images: {time_estimate['total_images']}")
    print(f"   Estimated time: {time_estimate['estimated_minutes']:.1f} minutes")
    
    # å®Ÿé¨“å®Ÿè¡Œ
    experiment = PilotExperiment(max_images_per_split=10)
    results = experiment.run_experiment()
    
    # æ¬¡ã‚¹ãƒ†ãƒƒãƒ—æ¨å¥¨
    successful_metrics = len([r for r in results.values() if 'error' not in r])
    
    if successful_metrics == len(results):
        print("\nğŸ‰ Pilot experiment successful!")
        print("ğŸ’¡ Next step: Run medium-scale experiment with:")
        print("   NIXPKGS_ALLOW_UNFREE=1 nix develop --impure -c uv run python experiments/fid_comparison/medium_scale_experiment.py")
    else:
        print(f"\nâš ï¸ {len(results) - successful_metrics} metrics failed")
        print("ğŸ”§ Please check logs and fix issues before proceeding")

if __name__ == "__main__":
    main()
```

### 3. experiments/fid_comparison/medium_scale_experiment.py
```python
#!/usr/bin/env python3
"""
FIDæœ€é©åŒ–å®Ÿé¨“: ä¸­è¦æ¨¡ç‰ˆ
çµ±è¨ˆçš„æœ‰æ„æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®ä¸­è¦æ¨¡å®Ÿé¨“
"""

import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

from utils.experiment_utils import ExperimentRunner, ExperimentValidator
from generative_latent_optimization import OptimizationConfig

class MediumScaleExperiment(ExperimentRunner):
    """ä¸­è¦æ¨¡FIDå®Ÿé¨“ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_images_per_split=50):
        super().__init__(
            experiment_name="medium_scale_fid",
            output_dir=Path(__file__).parent / "results" / "medium",
            max_images_per_split=max_images_per_split
        )
        
        # å®Ÿè£…æ¸ˆã¿æŒ‡æ¨™ï¼ˆæ‹¡å¼µå¾Œã¯å…¨æŒ‡æ¨™è¿½åŠ ï¼‰
        self.test_metrics = ['mse', 'l1']  # TODO: æ‹¡å¼µå¾Œã¯ ['mse', 'l1', 'lpips', 'ssim', 'improved_ssim', 'psnr']
    
    def run_experiment(self):
        """ä¸­è¦æ¨¡å®Ÿé¨“å®Ÿè¡Œ"""
        
        self.logger.log_experiment_start({
            'experiment_type': 'medium_scale',
            'max_images_per_split': self.max_images,
            'metrics': self.test_metrics
        })
        
        experiment_results = {}
        
        for metric in self.test_metrics:
            # ã‚ˆã‚Šè©³ç´°ãªæœ€é©åŒ–è¨­å®š
            config = OptimizationConfig(
                iterations=100,  # ä¸­è¦æ¨¡ã§ã¯å°‘ã—é•·ã‚
                learning_rate=0.4,
                loss_function=metric,
                device='cuda',
                checkpoint_interval=20
            )
            
            result = self.run_metric_experiment(metric, config)
            experiment_results[metric] = result
        
        # çµæœä¿å­˜
        results_file = self.results_manager.save_experiment_results(
            "medium_scale", experiment_results
        )
        
        # çµ±è¨ˆåˆ†æ
        self._perform_statistical_analysis(experiment_results)
        
        return experiment_results
    
    def _perform_statistical_analysis(self, results):
        """ä¸­è¦æ¨¡å®Ÿé¨“ã§ã®çµ±è¨ˆåˆ†æ"""
        
        # FIDã‚¹ã‚³ã‚¢æŠ½å‡º
        fid_scores = {
            metric: result['fid_score'] 
            for metric, result in results.items()
            if 'fid_score' in result
        }
        
        if len(fid_scores) < 2:
            logger.warning("Insufficient results for statistical analysis")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        best_metric = min(fid_scores.keys(), key=lambda k: fid_scores[k])
        worst_metric = max(fid_scores.keys(), key=lambda k: fid_scores[k])
        
        improvement = fid_scores[worst_metric] - fid_scores[best_metric]
        improvement_percent = (improvement / fid_scores[worst_metric]) * 100
        
        analysis = {
            'best_metric': best_metric,
            'best_fid': fid_scores[best_metric],
            'worst_metric': worst_metric,
            'worst_fid': fid_scores[worst_metric],
            'absolute_improvement': improvement,
            'relative_improvement_percent': improvement_percent
        }
        
        # åˆ†æçµæœä¿å­˜
        analysis_file = self.output_dir / "results" / "statistical_analysis.json"
        with open(analysis_file, 'w') as f:
            json.dump(analysis, f, indent=2)
        
        logger.info(f"Statistical analysis saved to: {analysis_file}")
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print(f"\nğŸ“ˆ Medium-Scale Statistical Analysis:")
        print(f"   ğŸ¥‡ Best metric: {best_metric} (FID: {fid_scores[best_metric]:.2f})")
        print(f"   ğŸ”´ Worst metric: {worst_metric} (FID: {fid_scores[worst_metric]:.2f})")
        print(f"   ğŸ“Š Improvement: {improvement:.2f} ({improvement_percent:.1f}%)")

def main():
    """ä¸­è¦æ¨¡å®Ÿé¨“ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    # ç’°å¢ƒæ¤œè¨¼
    issues = ExperimentValidator.validate_environment()
    if issues:
        print("âš ï¸ Environment issues:")
        for issue in issues:
            print(f"   - {issue}")
        return
    
    # å®Ÿé¨“å®Ÿè¡Œ
    experiment = MediumScaleExperiment(max_images_per_split=50)
    
    # æ™‚é–“æ¨å®šè¡¨ç¤º
    time_estimate = ExperimentValidator.estimate_experiment_time(50, 2)
    print(f"ğŸ“Š Estimated experiment time: {time_estimate['estimated_hours']:.1f} hours")
    
    # å®Ÿè¡Œç¢ºèª
    response = input("Continue with medium-scale experiment? (y/N): ")
    if response.lower() != 'y':
        print("Experiment cancelled")
        return
    
    # å®Ÿé¨“å®Ÿè¡Œ
    results = experiment.run_experiment()
    
    print(f"\nâœ… Medium-scale experiment completed!")
    print(f"ğŸ’¡ Next: Run full-scale experiment for definitive results")

if __name__ == "__main__":
    main()
```

### 4. experiments/fid_comparison/analyze_results.py
```python
#!/usr/bin/env python3
"""
FIDå®Ÿé¨“çµæœåˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List
from datetime import datetime

class FIDExperimentAnalyzer:
    """FIDå®Ÿé¨“çµæœåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, results_dir):
        self.results_dir = Path(results_dir)
        self.output_dir = self.results_dir / "analysis"
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def analyze_all_experiments(self):
        """å…¨å®Ÿé¨“çµæœã®çµ±åˆåˆ†æ"""
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        pilot_results = self._load_latest_results("pilot")
        medium_results = self._load_latest_results("medium_scale")
        full_results = self._load_latest_results("full_scale")
        
        # çµ±åˆåˆ†æ
        analysis = {
            'pilot': pilot_results,
            'medium': medium_results,
            'full': full_results,
            'summary': self._create_comprehensive_summary(pilot_results, medium_results, full_results)
        }
        
        # å¯è¦–åŒ–ç”Ÿæˆ
        self._generate_visualizations(analysis)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_report(analysis)
        
        return analysis
    
    def _load_latest_results(self, experiment_type):
        """æœ€æ–°ã®å®Ÿé¨“çµæœèª­ã¿è¾¼ã¿"""
        
        pattern = f"*{experiment_type}_results_*.json"
        result_files = list(self.results_dir.glob(pattern))
        
        if not result_files:
            return None
        
        latest_file = max(result_files, key=lambda f: f.stat().st_mtime)
        
        with open(latest_file, 'r') as f:
            return json.load(f)
    
    def _create_comprehensive_summary(self, pilot, medium, full):
        """åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        
        summary = {
            'experiment_progression': {},
            'metric_rankings': {},
            'statistical_significance': {}
        }
        
        # å„æ®µéšã§ã®çµæœæ¯”è¼ƒ
        for stage, results in [('pilot', pilot), ('medium', medium), ('full', full)]:
            if results and 'results' in results:
                fid_scores = {
                    metric: data['fid_score'] 
                    for metric, data in results['results'].items()
                    if 'fid_score' in data
                }
                
                summary['experiment_progression'][stage] = {
                    'fid_scores': fid_scores,
                    'best_metric': min(fid_scores.keys(), key=lambda k: fid_scores[k]) if fid_scores else None,
                    'worst_metric': max(fid_scores.keys(), key=lambda k: fid_scores[k]) if fid_scores else None
                }
        
        return summary
    
    def _generate_visualizations(self, analysis):
        """åˆ†æçµæœã®å¯è¦–åŒ–"""
        
        # å®Ÿé¨“æ®µéšåˆ¥FIDæ¯”è¼ƒ
        self._plot_experiment_progression(analysis)
        
        # æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµæœï¼‰
        if analysis['full']:
            self._plot_final_ranking(analysis['full'])
    
    def _plot_experiment_progression(self, analysis):
        """å®Ÿé¨“æ®µéšåˆ¥ã®é€²æ—å¯è¦–åŒ–"""
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        stages = ['pilot', 'medium', 'full']
        
        for i, stage in enumerate(stages):
            ax = axes[i]
            
            if analysis[stage] and 'experiment_progression' in analysis['summary']:
                stage_data = analysis['summary']['experiment_progression'].get(stage)
                if stage_data and 'fid_scores' in stage_data:
                    fid_scores = stage_data['fid_scores']
                    
                    metrics = list(fid_scores.keys())
                    scores = list(fid_scores.values())
                    
                    bars = ax.bar(metrics, scores, color=sns.color_palette("husl", len(metrics)))
                    ax.set_title(f'{stage.title()} Experiment')
                    ax.set_ylabel('FID Score')
                    
                    # æ•°å€¤è¡¨ç¤º
                    for bar, score in zip(bars, scores):
                        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                               f'{score:.1f}', ha='center', va='bottom')
            else:
                ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{stage.title()} Experiment (No Data)')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'experiment_progression.png', dpi=300)
        plt.close()
    
    def _plot_final_ranking(self, full_results):
        """æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®å¯è¦–åŒ–"""
        
        if 'results' not in full_results:
            return
        
        fid_scores = {
            metric: data['fid_score']
            for metric, data in full_results['results'].items()
            if 'fid_score' in data
        }
        
        # FIDé †ã§ã‚½ãƒ¼ãƒˆ
        sorted_items = sorted(fid_scores.items(), key=lambda x: x[1])
        
        metrics = [item[0] for item in sorted_items]
        scores = [item[1] for item in sorted_items]
        
        plt.figure(figsize=(12, 8))
        colors = plt.cm.RdYlGn_r(np.linspace(0.3, 0.8, len(metrics)))
        bars = plt.bar(metrics, scores, color=colors)
        
        plt.title('Final FID Score Ranking (Full Dataset)', fontsize=16, fontweight='bold')
        plt.ylabel('FID Score (Lower = Better Quality)', fontsize=12)
        plt.xlabel('Optimization Metric', fontsize=12)
        
        # æ•°å€¤è¡¨ç¤ºã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        for i, (bar, score) in enumerate(zip(bars, scores)):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{score:.1f}\\n(#{i+1})', ha='center', va='bottom', fontweight='bold')
        
        plt.xticks(rotation=45)
        plt.grid(axis='y', alpha=0.3)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'final_fid_ranking.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_report(self, analysis):
        """å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        report_content = f"""
# FIDæœ€é©åŒ–å®Ÿé¨“ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## ğŸ“Š å®Ÿé¨“æ¦‚è¦

æœ¬å®Ÿé¨“ã§ã¯ã€BSDS500ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ç•°ãªã‚‹æœ€é©åŒ–æŒ‡æ¨™ã§ã®VAEæ½œåœ¨è¡¨ç¾æœ€é©åŒ–ã‚’è¡Œã„ã€
å„æŒ‡æ¨™ãŒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ¬ãƒ™ãƒ«ã®FIDã‚¹ã‚³ã‚¢ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®šé‡çš„ã«åˆ†æã—ãŸã€‚

## ğŸ¯ å®Ÿé¨“æ®µéšåˆ¥çµæœ

"""
        
        # å„æ®µéšã®çµæœã‚µãƒãƒªãƒ¼
        stages = ['pilot', 'medium', 'full']
        for stage in stages:
            if analysis[stage]:
                results = analysis[stage].get('results', {})
                if results:
                    report_content += f"### {stage.title()} Experiment\\n"
                    
                    for metric, data in results.items():
                        if 'fid_score' in data:
                            fid = data['fid_score']
                            time_min = data.get('processing_time_seconds', 0) / 60
                            report_content += f"- **{metric.upper()}**: FID = {fid:.2f} (å‡¦ç†æ™‚é–“: {time_min:.1f}åˆ†)\\n"
                    
                    report_content += "\\n"
        
        # çµè«–
        if analysis['full']:
            full_fid = analysis['full'].get('results', {})
            if full_fid:
                best_metric = min(full_fid.keys(), key=lambda k: full_fid[k].get('fid_score', float('inf')))
                worst_metric = max(full_fid.keys(), key=lambda k: full_fid[k].get('fid_score', 0))
                
                report_content += f"""
## ğŸ† æœ€çµ‚çµè«–

### æœ€å„ªç§€æŒ‡æ¨™
- **{best_metric.upper()}**: FID = {full_fid[best_metric]['fid_score']:.2f}
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªãŒæœ€ã‚‚è‰¯å¥½ã«ä¿ãŸã‚Œã‚‹æœ€é©åŒ–æ‰‹æ³•

### æœ€åŠ£ä½æŒ‡æ¨™  
- **{worst_metric.upper()}**: FID = {full_fid[worst_metric]['fid_score']:.2f}
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå“è³ªã®åŠ£åŒ–ãŒæœ€ã‚‚é¡•è‘—ãªæœ€é©åŒ–æ‰‹æ³•

### æ¨å¥¨äº‹é …
1. é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã«ã¯ **{best_metric}** æœ€é©åŒ–ã‚’æ¨å¥¨
2. **{worst_metric}** æœ€é©åŒ–ã¯FIDè¦³ç‚¹ã‹ã‚‰æ¨å¥¨ã—ãªã„
3. ç”¨é€”ã«å¿œã˜ãŸæœ€é©åŒ–æŒ‡æ¨™ã®é¸æŠãŒé‡è¦

## ğŸ“ˆ è©³ç´°åˆ†æ

è©³ç´°ãªåˆ†æçµæœã¯ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ï¼š
- `final_fid_ranking.png`: æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°å¯è¦–åŒ–
- `experiment_progression.png`: å®Ÿé¨“æ®µéšåˆ¥é€²æ—
"""
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = self.output_dir / "experiment_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“„ Analysis report generated: {report_file}")

def main():
    """åˆ†æãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    analyzer = FIDExperimentAnalyzer(Path(__file__).parent / "results")
    analysis = analyzer.analyze_all_experiments()
    
    print("ğŸ“Š FID Experiment Analysis Completed!")
    print(f"ğŸ“ Results saved in: {analyzer.output_dir}")
    print("\nFiles generated:")
    print("   - experiment_report.md")
    print("   - final_fid_ranking.png")
    print("   - experiment_progression.png")

if __name__ == "__main__":
    main()
```

## âš¡ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ä¸€è¦§

### æ®µéšçš„å®Ÿé¨“å®Ÿè¡Œ
```bash
# 1. ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“ï¼ˆç´„1æ™‚é–“ï¼‰
NIXPKGS_ALLOW_UNFREE=1 nix develop --impure -c uv run python experiments/fid_comparison/pilot_experiment.py

# 2. ä¸­è¦æ¨¡å®Ÿé¨“ï¼ˆç´„8æ™‚é–“ï¼‰
NIXPKGS_ALLOW_UNFREE=1 nix develop --impure -c uv run python experiments/fid_comparison/medium_scale_experiment.py

# 3. å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“ï¼ˆç´„48æ™‚é–“ï¼‰
NIXPKGS_ALLOW_UNFREE=1 nix develop --impure -c uv run python experiments/fid_comparison/full_scale_experiment.py

# 4. çµæœåˆ†æ
NIXPKGS_ALLOW_UNFREE=1 nix develop --impure -c uv run python experiments/fid_comparison/analyze_results.py
```

### ç’°å¢ƒå¤‰æ•°è¨­å®šä¾‹
```bash
# å¿…é ˆç’°å¢ƒå¤‰æ•°
export BSDS500_PATH="/path/to/bsds500/dataset"
export HF_TOKEN="your_huggingface_token"

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ç’°å¢ƒå¤‰æ•°
export CUDA_VISIBLE_DEVICES=0,1  # ãƒãƒ«ãƒGPUä½¿ç”¨æ™‚
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
```

### ä¸¦åˆ—å®Ÿé¨“å®Ÿè¡Œï¼ˆ2GPUç’°å¢ƒï¼‰
```bash
# GPU0: è»½é‡æŒ‡æ¨™
CUDA_VISIBLE_DEVICES=0 NIXPKGS_ALLOW_UNFREE=1 nix develop --impure -c uv run python \
  experiments/fid_comparison/parallel_light_metrics.py &

# GPU1: é‡ã„æŒ‡æ¨™
CUDA_VISIBLE_DEVICES=1 NIXPKGS_ALLOW_UNFREE=1 nix develop --impure -c uv run python \
  experiments/fid_comparison/parallel_heavy_metrics.py &

# ä¸¡æ–¹ã®å®Œäº†ã‚’å¾…æ©Ÿ
wait
```

## ğŸ“‹ å®Ÿè¡Œå‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ç’°å¢ƒæº–å‚™
- [ ] BSDS500ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»é…ç½®
- [ ] HF_TOKENã®è¨­å®š
- [ ] å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ç¢ºèªï¼ˆlpipsã€torchmetricsãªã©ï¼‰
- [ ] ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å®¹é‡ç¢ºèªï¼ˆ100GBä»¥ä¸Šæ¨å¥¨ï¼‰

### å®Ÿè£…æº–å‚™
- [ ] LatentOptimizerã®æå¤±é–¢æ•°æ‹¡å¼µå®Ÿè£…
- [ ] æ–°æå¤±é–¢æ•°ã®å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ

### å®Ÿè¡Œæº–å‚™
- [ ] ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“ã®æˆåŠŸç¢ºèª
- [ ] å‡¦ç†æ™‚é–“ã®å¦¥å½“æ€§ç¢ºèª
- [ ] ä¸­è¦æ¨¡å®Ÿé¨“ã§ã®çµ±è¨ˆçš„å‚¾å‘ç¢ºèª
- [ ] å…¨å®Ÿé¨“å®Ÿè¡Œã®æœ€çµ‚æ±ºå®š

---

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¨­è¨ˆã«ã‚ˆã‚Šã€æ®µéšçš„ã‹ã¤åŠ¹ç‡çš„ãªFIDæœ€é©åŒ–å®Ÿé¨“ã®å®Ÿè¡ŒãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚å„æ®µéšã§çµæœã‚’æ¤œè¨¼ã—ãªãŒã‚‰ã€æœ€çµ‚çš„ã«ç§‘å­¦çš„ã«å¦¥å½“ãªçµè«–ã‚’ç²å¾—ã§ãã‚‹å®Ÿé¨“ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚