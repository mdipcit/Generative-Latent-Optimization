# æå¤±é–¢æ•°æ‹¡å¼µè¨­è¨ˆ: LPIPSãƒ»SSIMãƒ»Improved SSIMãƒ»PSNRå¯¾å¿œ

## ğŸ¯ æ‹¡å¼µç›®çš„

ç¾åœ¨MSEã¨L1ã®ã¿ã«å¯¾å¿œã—ã¦ã„ã‚‹LatentOptimizerã®æå¤±é–¢æ•°ã‚’ã€LPIPSã€SSIMã€Improved SSIMã€PSNRã«æ‹¡å¼µã—ã€FIDå®Ÿé¨“ã§ã®å„æŒ‡æ¨™ã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

## ğŸ“‹ ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³

### æ—¢å­˜ã® _calculate_batch_loss ãƒ¡ã‚½ãƒƒãƒ‰
```python
# src/generative_latent_optimization/optimization/latent_optimizer.py:322-346

def _calculate_batch_loss(self, targets: torch.Tensor, reconstructed: torch.Tensor) -> torch.Tensor:
    if self.config.loss_function == 'mse':
        losses = torch.nn.functional.mse_loss(
            targets, reconstructed, reduction='none'
        ).mean(dim=(1, 2, 3))
    elif self.config.loss_function == 'l1':
        losses = torch.nn.functional.l1_loss(
            targets, reconstructed, reduction='none'
        ).mean(dim=(1, 2, 3))
    else:
        raise ValueError(f"Unsupported loss function: {self.config.loss_function}")
    
    return losses
```

## ğŸ”§ æ‹¡å¼µå®Ÿè£…è¨­è¨ˆ

### 1. LPIPSæå¤±å®Ÿè£…

#### è¨­è¨ˆæ–¹é‡
- LPIPSMetricã‚’åˆ©ç”¨ã—ãŸãƒãƒƒãƒå¯¾å¿œæå¤±è¨ˆç®—
- å‹¾é…è¨ˆç®—å¯èƒ½ãªå®Ÿè£…
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†

#### å®Ÿè£…æ¡ˆ
```python
def _calculate_lpips_batch_loss(self, targets: torch.Tensor, reconstructed: torch.Tensor) -> torch.Tensor:
    """LPIPSæå¤±ã‚’ãƒãƒƒãƒã§è¨ˆç®—ï¼ˆå‹¾é…è¨ˆç®—å¯¾å¿œï¼‰"""
    
    # LPIPSãƒ¡ãƒˆãƒªã‚¯ãŒæœªåˆæœŸåŒ–ã®å ´åˆã¯åˆæœŸåŒ–
    if not hasattr(self, '_lpips_metric'):
        from ..metrics.individual_metrics import LPIPSMetric
        self._lpips_metric = LPIPSMetric(device=self.device)
    
    # ãƒãƒƒãƒå‡¦ç†: å„ç”»åƒãƒšã‚¢ã«å¯¾ã—ã¦LPIPSè¨ˆç®—
    batch_size = targets.shape[0]
    lpips_losses = []
    
    for i in range(batch_size):
        # å€‹åˆ¥ç”»åƒã®LPIPSè¨ˆç®—
        target_single = targets[i:i+1]  # [1, C, H, W]
        recon_single = reconstructed[i:i+1]  # [1, C, H, W]
        
        # LPIPSè¨ˆç®—ï¼ˆå‹¾é…æœ‰åŠ¹ï¼‰
        with torch.enable_grad():
            # ç¯„å›²èª¿æ•´: [0,1] â†’ [-1,1] (LPIPSè¦æ±‚)
            target_normalized = target_single * 2.0 - 1.0
            recon_normalized = recon_single * 2.0 - 1.0
            
            # LPIPSãƒ¡ãƒˆãƒªãƒƒã‚¯ã®å†…éƒ¨è¨ˆç®—ã‚’ç›´æ¥åˆ©ç”¨
            lpips_value = self._lpips_metric.loss_fn(target_normalized, recon_normalized)
            lpips_losses.append(lpips_value.squeeze())
    
    return torch.stack(lpips_losses)
```

### 2. SSIMæå¤±å®Ÿè£…

#### è¨­è¨ˆæ–¹é‡
- SSIMã‚’æå¤±ã«å¤‰æ›: `loss = 1.0 - ssim`
- å‹¾é…è¨ˆç®—å¯èƒ½ãªSSIMå®Ÿè£…
- ãƒãƒƒãƒåŠ¹ç‡çš„ãªè¨ˆç®—

#### å®Ÿè£…æ¡ˆ
```python
def _calculate_ssim_batch_loss(self, targets: torch.Tensor, reconstructed: torch.Tensor) -> torch.Tensor:
    """SSIMæå¤±ã‚’ãƒãƒƒãƒã§è¨ˆç®—ï¼ˆ1 - SSIMã§æå¤±ã«å¤‰æ›ï¼‰"""
    
    from torchmetrics.image import StructuralSimilarityIndexMeasure
    
    # SSIMãƒ¡ãƒˆãƒªã‚¯ãŒæœªåˆæœŸåŒ–ã®å ´åˆã¯åˆæœŸåŒ–
    if not hasattr(self, '_ssim_metric'):
        self._ssim_metric = StructuralSimilarityIndexMeasure(
            data_range=1.0,  # [0, 1]ç¯„å›²
            gaussian_kernel=True,
            kernel_size=11,
            sigma=1.5,
            reduction='none'  # ãƒãƒƒãƒæ¯ã®çµæœã‚’å–å¾—
        ).to(self.device)
    
    # SSIMè¨ˆç®—ï¼ˆå‹¾é…æœ‰åŠ¹ï¼‰
    with torch.enable_grad():
        ssim_values = self._ssim_metric(targets, reconstructed)
        # SSIMæå¤± = 1 - SSIMï¼ˆé«˜ã„SSIMã§ä½ã„æå¤±ï¼‰
        ssim_losses = 1.0 - ssim_values
    
    return ssim_losses
```

### 3. Improved SSIMæå¤±å®Ÿè£…

#### è¨­è¨ˆæ–¹é‡
- ImprovedSSIMã‚¯ãƒ©ã‚¹ã®å†…éƒ¨å®Ÿè£…ã‚’ç›´æ¥åˆ©ç”¨
- å‹¾é…è¨ˆç®—å¯¾å¿œã®è¨­è¨ˆ
- æ¨™æº–SSIMã¨ã®å·®åˆ¥åŒ–

#### å®Ÿè£…æ¡ˆ
```python
def _calculate_improved_ssim_batch_loss(self, targets: torch.Tensor, reconstructed: torch.Tensor) -> torch.Tensor:
    """Improved SSIMæå¤±ã‚’ãƒãƒƒãƒã§è¨ˆç®—"""
    
    # Improved SSIMãƒ¡ãƒˆãƒªã‚¯ãŒæœªåˆæœŸåŒ–ã®å ´åˆã¯åˆæœŸåŒ–
    if not hasattr(self, '_improved_ssim_metric'):
        from ..metrics.individual_metrics import ImprovedSSIM
        self._improved_ssim_metric = ImprovedSSIM(device=self.device)
    
    # ãƒãƒƒãƒå‡¦ç†ã§ã®è¨ˆç®—
    batch_size = targets.shape[0]
    ssim_losses = []
    
    for i in range(batch_size):
        target_single = targets[i:i+1]
        recon_single = reconstructed[i:i+1]
        
        # Improved SSIMè¨ˆç®—ï¼ˆå‹¾é…æœ‰åŠ¹ï¼‰
        with torch.enable_grad():
            # ImprovedSSIMã®å†…éƒ¨SSIMãƒ¡ãƒˆãƒªã‚¯ã‚’ç›´æ¥åˆ©ç”¨
            ssim_value = self._improved_ssim_metric.ssim(target_single, recon_single)
            ssim_loss = 1.0 - ssim_value
            ssim_losses.append(ssim_loss)
    
    return torch.stack(ssim_losses)
```

### 4. PSNRæå¤±å®Ÿè£…

#### è¨­è¨ˆæ–¹é‡
- PSNRæœ€å¤§åŒ– = MSEæœ€å°åŒ–ã®ç­‰ä¾¡æ€§åˆ©ç”¨
- å¯¾æ•°æ¼”ç®—ã«ã‚ˆã‚‹å‹¾é…è¨ˆç®—ã®å®‰å®šæ€§ç¢ºä¿
- æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®ã‚¯ãƒ©ãƒ³ãƒ—å‡¦ç†

#### å®Ÿè£…æ¡ˆ
```python
def _calculate_psnr_batch_loss(self, targets: torch.Tensor, reconstructed: torch.Tensor) -> torch.Tensor:
    """PSNRæå¤±ã‚’ãƒãƒƒãƒã§è¨ˆç®—ï¼ˆ-PSNRã§æå¤±ã«å¤‰æ›ï¼‰"""
    
    # MSEè¨ˆç®—
    mse_values = torch.nn.functional.mse_loss(
        targets, reconstructed, reduction='none'
    ).mean(dim=(1, 2, 3))  # [B]
    
    # PSNRãƒ™ãƒ¼ã‚¹æå¤±è¨ˆç®—
    with torch.enable_grad():
        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®ã‚¯ãƒ©ãƒ³ãƒ—
        mse_clamped = torch.clamp(mse_values, min=1e-10)
        
        # PSNR = 20 * log10(MAX_VAL) - 10 * log10(MSE)
        # MAX_VAL = 1.0 (ç”»åƒãŒ[0,1]ç¯„å›²ã®å ´åˆ)
        psnr_values = 20 * torch.log10(torch.tensor(1.0)) - 10 * torch.log10(mse_clamped)
        
        # PSNRæå¤± = -PSNRï¼ˆé«˜ã„PSNRã§ä½ã„æå¤±ï¼‰
        psnr_losses = -psnr_values
    
    return psnr_losses
```

## ğŸ”„ çµ±åˆå®Ÿè£…: æ‹¡å¼µã•ã‚ŒãŸ _calculate_batch_loss

### å®Œå…¨ãªå®Ÿè£…
```python
def _calculate_batch_loss(self, targets: torch.Tensor, reconstructed: torch.Tensor) -> torch.Tensor:
    """
    Calculate loss for batch with extended metrics support
    
    Args:
        targets: Target images [B, C, H, W] in [0, 1] range
        reconstructed: Reconstructed images [B, C, H, W] in [0, 1] range
        
    Returns:
        Per-sample losses [B]
    """
    loss_function = self.config.loss_function
    
    if loss_function == 'mse':
        losses = torch.nn.functional.mse_loss(
            targets, reconstructed, reduction='none'
        ).mean(dim=(1, 2, 3))
        
    elif loss_function == 'l1':
        losses = torch.nn.functional.l1_loss(
            targets, reconstructed, reduction='none'
        ).mean(dim=(1, 2, 3))
        
    elif loss_function == 'lpips':
        losses = self._calculate_lpips_batch_loss(targets, reconstructed)
        
    elif loss_function == 'ssim':
        losses = self._calculate_ssim_batch_loss(targets, reconstructed)
        
    elif loss_function == 'improved_ssim':
        losses = self._calculate_improved_ssim_batch_loss(targets, reconstructed)
        
    elif loss_function == 'psnr':
        losses = self._calculate_psnr_batch_loss(targets, reconstructed)
        
    else:
        raise ValueError(f"Unsupported loss function: {loss_function}")
    
    return losses
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè£…è¨ˆç”»

### 1. å˜ä½“ãƒ†ã‚¹ãƒˆ
```python
# tests/unit/test_optimization/test_extended_loss_functions.py

class TestExtendedLossFunctions:
    def test_lpips_loss_calculation(self):
        """LPIPSæå¤±è¨ˆç®—ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        # åŒä¸€ç”»åƒã®LPIPSæå¤±ã¯0ã«è¿‘ã„
        # ç•°ãªã‚‹ç”»åƒã®LPIPSæå¤±ã¯æ­£ã®å€¤
        
    def test_ssim_loss_calculation(self):
        """SSIMæå¤±è¨ˆç®—ã®æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ"""
        # åŒä¸€ç”»åƒã®SSIMæå¤±ã¯0ã«è¿‘ã„
        # ç™½ç”»åƒã¨é»’ç”»åƒã®SSIMæå¤±ã¯1ã«è¿‘ã„
        
    def test_gradient_computation(self):
        """å„æå¤±é–¢æ•°ã§ã®å‹¾é…è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        # requires_grad=Trueã§ã®å‹¾é…è¨ˆç®—ç¢ºèª
        
    def test_batch_consistency(self):
        """ãƒãƒƒãƒå‡¦ç†ã¨å€‹åˆ¥å‡¦ç†ã®ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        # ãƒãƒƒãƒå‡¦ç†çµæœ = å€‹åˆ¥å‡¦ç†çµæœã®çµ„ã¿åˆã‚ã›
```

### 2. çµ±åˆãƒ†ã‚¹ãƒˆ
```python
# tests/integration/test_optimization_extended.py

class TestExtendedOptimizationIntegration:
    def test_end_to_end_optimization_all_metrics(self):
        """å…¨æŒ‡æ¨™ã§ã®æœ€é©åŒ–ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        for metric in ['mse', 'l1', 'lpips', 'ssim', 'improved_ssim', 'psnr']:
            config = OptimizationConfig(loss_function=metric)
            optimizer = LatentOptimizer(config)
            # å°ã•ãªãƒ†ã‚¹ãƒˆç”»åƒã§æœ€é©åŒ–å®Ÿè¡Œ
            
    def test_convergence_behavior(self):
        """å„æŒ‡æ¨™ã§ã®åæŸæŒ™å‹•ãƒ†ã‚¹ãƒˆ"""
        # åæŸé€Ÿåº¦ã¨æœ€çµ‚å“è³ªã®æ¯”è¼ƒ
```

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. è¨ˆç®—åŠ¹ç‡åŒ–

#### LPIPSåŠ¹ç‡åŒ–
```python
class LPIPSBatchOptimizer:
    """LPIPSè¨ˆç®—ã®åŠ¹ç‡åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, device, batch_size=8):
        self.device = device
        self.batch_size = batch_size
        import lpips
        self.loss_fn = lpips.LPIPS(net='alex', verbose=False).to(device)
    
    def calculate_batch_loss(self, targets, reconstructed):
        """åŠ¹ç‡çš„ãªãƒãƒƒãƒLPIPSè¨ˆç®—"""
        # å°ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶å¾¡
        total_batch_size = targets.shape[0]
        all_losses = []
        
        for i in range(0, total_batch_size, self.batch_size):
            end_idx = min(i + self.batch_size, total_batch_size)
            batch_targets = targets[i:end_idx]
            batch_recons = reconstructed[i:end_idx]
            
            # ç¯„å›²å¤‰æ›
            batch_targets_norm = batch_targets * 2.0 - 1.0
            batch_recons_norm = batch_recons * 2.0 - 1.0
            
            # LPIPSè¨ˆç®—
            batch_losses = self.loss_fn(batch_targets_norm, batch_recons_norm)
            all_losses.append(batch_losses.squeeze(-1).squeeze(-1))
        
        return torch.cat(all_losses)
```

### 2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–

#### ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è“„ç©æˆ¦ç•¥
```python
def _calculate_memory_efficient_loss(self, targets, reconstructed, accumulation_steps=4):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªæå¤±è¨ˆç®—ï¼ˆã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è“„ç©ï¼‰"""
    
    batch_size = targets.shape[0]
    micro_batch_size = max(1, batch_size // accumulation_steps)
    
    total_loss = 0
    for i in range(0, batch_size, micro_batch_size):
        end_idx = min(i + micro_batch_size, batch_size)
        
        micro_targets = targets[i:end_idx]
        micro_recons = reconstructed[i:end_idx]
        
        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã§ã®æå¤±è¨ˆç®—
        micro_losses = self._calculate_batch_loss(micro_targets, micro_recons)
        micro_loss = micro_losses.mean() / accumulation_steps
        
        total_loss += micro_loss
    
    return total_loss
```

### 3. æ•°å€¤å®‰å®šæ€§

#### å®‰å®šåŒ–å®Ÿè£…
```python
def _calculate_numerically_stable_psnr_loss(self, targets, reconstructed):
    """æ•°å€¤çš„ã«å®‰å®šãªPSNRæå¤±è¨ˆç®—"""
    
    # MSEè¨ˆç®—
    mse_values = torch.nn.functional.mse_loss(
        targets, reconstructed, reduction='none'
    ).mean(dim=(1, 2, 3))
    
    # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®å‡¦ç†
    epsilon = 1e-10
    mse_stable = torch.clamp(mse_values, min=epsilon)
    
    # PSNRè¨ˆç®—ï¼ˆlog10ã®å®‰å®šåŒ–ï¼‰
    with torch.enable_grad():
        # log10ã®ä»£ã‚ã‚Šã«logã‚’ä½¿ç”¨ã—ã¦æ•°å€¤å®‰å®šæ€§å‘ä¸Š
        log_mse = torch.log(mse_stable)
        psnr_values = 20 * torch.log10(torch.tensor(1.0, device=self.device)) - 10 * log_mse / torch.log(torch.tensor(10.0, device=self.device))
        
        # PSNRæå¤±ï¼ˆè² å€¤ã§æå¤±ã«å¤‰æ›ï¼‰
        psnr_losses = -psnr_values
    
    return psnr_losses
```

## ğŸ“Š å®Ÿè£…æ®µéšè¨ˆç”»

### Stage 1: åŸºæœ¬å®Ÿè£…
1. **LPIPSæå¤±**: å€‹åˆ¥ç”»åƒãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…
2. **SSIMæå¤±**: TorchMetricsä½¿ç”¨ã®å®Ÿè£…
3. **åŸºæœ¬ãƒ†ã‚¹ãƒˆ**: å‹•ä½œç¢ºèªãƒ¬ãƒ™ãƒ«ã®ãƒ†ã‚¹ãƒˆ

### Stage 2: åŠ¹ç‡åŒ–å®Ÿè£…
1. **ãƒãƒƒãƒåŠ¹ç‡åŒ–**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
2. **ä¸¦åˆ—å‡¦ç†**: ãƒãƒ«ãƒGPUå¯¾å¿œ
3. **æ•°å€¤å®‰å®šæ€§**: æ¥µç«¯ã‚±ãƒ¼ã‚¹ã§ã®å®‰å®šæ€§ç¢ºä¿

### Stage 3: é«˜åº¦å®Ÿè£…  
1. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æå¤±**: è¤‡æ•°æŒ‡æ¨™ã®åŠ é‡å¹³å‡
2. **é©å¿œçš„é‡ã¿**: æœ€é©åŒ–é€²è¡Œã«å¿œã˜ãŸé‡ã¿èª¿æ•´
3. **ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™**: ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©æå¤±é–¢æ•°ã‚µãƒãƒ¼ãƒˆ

## ğŸ” è©³ç´°å®Ÿè£…ä»•æ§˜

### OptimizationConfig ã®æ‹¡å¼µ

#### æ–°ã—ã„è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³
```python
@dataclass
class OptimizationConfig:
    iterations: int = 150
    learning_rate: float = 0.4
    loss_function: str = 'mse'  # 'mse', 'l1', 'lpips', 'ssim', 'improved_ssim', 'psnr'
    convergence_threshold: float = 1e-6
    checkpoint_interval: int = 20
    device: str = "cuda"
    
    # æ–°è¦è¿½åŠ : é«˜åº¦ãªè¨­å®š
    lpips_network: str = 'alex'  # LPIPSç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ('alex', 'vgg', 'squeeze')
    ssim_kernel_size: int = 11   # SSIMç”¨ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º
    ssim_sigma: float = 1.5      # SSIMã‚¬ã‚¦ã‚·ã‚¢ãƒ³Ïƒ
    numerical_stability: bool = True  # æ•°å€¤å®‰å®šæ€§æ©Ÿèƒ½
    memory_efficient: bool = True     # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒ¢ãƒ¼ãƒ‰
    gradient_accumulation_steps: int = 1  # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è“„ç©ã‚¹ãƒ†ãƒƒãƒ—
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–

#### å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç†
```python
def _safe_calculate_batch_loss(self, targets, reconstructed):
    """å®‰å…¨ãªæå¤±è¨ˆç®—ï¼ˆã‚¨ãƒ©ãƒ¼å›å¾©æ©Ÿèƒ½ä»˜ãï¼‰"""
    
    try:
        return self._calculate_batch_loss(targets, reconstructed)
        
    except RuntimeError as e:
        if "out of memory" in str(e):
            # ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®è‡ªå‹•å›å¾©
            torch.cuda.empty_cache()
            return self._calculate_memory_efficient_loss(targets, reconstructed)
        else:
            raise
            
    except ImportError as e:
        if "lpips" in str(e) and self.config.loss_function == 'lpips':
            # LPIPSãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯SSIMã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.warning("LPIPS not available, falling back to SSIM")
            old_loss_function = self.config.loss_function
            self.config.loss_function = 'ssim'
            result = self._calculate_batch_loss(targets, reconstructed)
            self.config.loss_function = old_loss_function  # å¾©å…ƒ
            return result
        else:
            raise
```

### ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½

#### æå¤±é–¢æ•°ã®å¦¥å½“æ€§æ¤œè¨¼
```python
def _validate_loss_function_setup(self):
    """æå¤±é–¢æ•°ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¤œè¨¼"""
    
    loss_function = self.config.loss_function
    
    # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
    if loss_function == 'lpips':
        try:
            import lpips
        except ImportError:
            raise ImportError("LPIPS loss requires lpips package: pip install lpips")
    
    elif loss_function in ['ssim', 'improved_ssim']:
        try:
            from torchmetrics.image import StructuralSimilarityIndexMeasure
        except ImportError:
            raise ImportError("SSIM loss requires torchmetrics package: pip install torchmetrics")
    
    # ãƒ‡ãƒã‚¤ã‚¹äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
    if not torch.cuda.is_available() and self.device == 'cuda':
        logger.warning("CUDA not available, some metrics may run slower on CPU")
    
    # æ•°å€¤ç¯„å›²ãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒ†ã‚¹ãƒˆãƒ†ãƒ³ã‚½ãƒ«
    test_targets = torch.rand(1, 3, 64, 64, device=self.device)
    test_recons = torch.rand(1, 3, 64, 64, device=self.device)
    
    try:
        test_loss = self._calculate_batch_loss(test_targets, test_recons)
        logger.info(f"Loss function '{loss_function}' validated successfully")
        
    except Exception as e:
        raise RuntimeError(f"Loss function '{loss_function}' validation failed: {e}")
```

## ğŸ¯ FIDå®Ÿé¨“ç‰¹åŒ–æ©Ÿèƒ½

### å®Ÿé¨“åˆ¶å¾¡ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

#### FIDExperimentController
```python
# experiments/fid_comparison/experiment_controller.py

class FIDExperimentController:
    """FIDæœ€é©åŒ–å®Ÿé¨“ã®åˆ¶å¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, bsds500_path, output_base_path):
        self.bsds500_path = bsds500_path
        self.output_base_path = Path(output_base_path)
        
        # å®Ÿé¨“å¯¾è±¡æŒ‡æ¨™
        self.loss_functions = [
            'mse', 'l1', 'lpips', 'ssim', 'improved_ssim', 'psnr'
        ]
        
        # FIDè©•ä¾¡å™¨
        self.fid_evaluator = DatasetFIDEvaluator(device='cuda')
    
    def run_complete_experiment(self, max_images_per_split=None):
        """å®Œå…¨ãªFIDæ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ"""
        
        experiment_results = {}
        
        for loss_func in self.loss_functions:
            logger.info(f"ğŸ”„ Starting optimization with {loss_func} loss")
            
            # æœ€é©åŒ–è¨­å®š
            config = OptimizationConfig(
                iterations=150,
                learning_rate=0.4,
                loss_function=loss_func,
                device='cuda'
            )
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€é©åŒ–å®Ÿè¡Œ
            result = self._optimize_and_evaluate_single_metric(
                config, max_images_per_split
            )
            
            experiment_results[loss_func] = result
            logger.info(f"âœ… {loss_func} optimization completed: FID = {result['fid_score']:.2f}")
        
        # çµæœæ¯”è¼ƒãƒ»åˆ†æ
        comparison_results = self._analyze_experiment_results(experiment_results)
        
        return {
            'individual_results': experiment_results,
            'comparison_analysis': comparison_results
        }
    
    def _optimize_and_evaluate_single_metric(self, config, max_images):
        """å˜ä¸€æŒ‡æ¨™ã§ã®æœ€é©åŒ–ã¨FIDè©•ä¾¡"""
        
        # å‡ºåŠ›ãƒ‘ã‚¹è¨­å®š
        output_dir = self.output_base_path / f"{config.loss_function}_optimized"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
        start_time = time.time()
        datasets = process_bsds500_dataset(
            self.bsds500_path,
            output_dir / "dataset",
            config,
            max_images_per_split=max_images,
            create_pytorch_dataset=True,
            create_png_dataset=True
        )
        processing_time = time.time() - start_time
        
        # FIDè©•ä¾¡å®Ÿè¡Œ
        fid_score = self.fid_evaluator.evaluate_created_dataset_vs_original(
            datasets['png'], 
            self.bsds500_path
        ).fid_score
        
        return {
            'fid_score': fid_score,
            'dataset_paths': datasets,
            'processing_time_seconds': processing_time,
            'optimization_config': config
        }
```

### çµæœåˆ†æã‚·ã‚¹ãƒ†ãƒ 

#### çµ±è¨ˆåˆ†ææ©Ÿèƒ½
```python
class FIDComparisonAnalyzer:
    """FIDå®Ÿé¨“çµæœã®åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def analyze_results(self, experiment_results):
        """å®Ÿé¨“çµæœã®åŒ…æ‹¬çš„åˆ†æ"""
        
        # FIDã‚¹ã‚³ã‚¢æŠ½å‡º
        fid_scores = {
            metric: result['fid_score'] 
            for metric, result in experiment_results.items()
        }
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
        fid_ranking = sorted(fid_scores.items(), key=lambda x: x[1])
        
        # çµ±è¨ˆåˆ†æ
        analysis = {
            'fid_scores': fid_scores,
            'ranking': fid_ranking,
            'best_metric': fid_ranking[0][0],
            'worst_metric': fid_ranking[-1][0],
            'score_range': fid_ranking[-1][1] - fid_ranking[0][1],
            'relative_improvements': self._calculate_relative_improvements(fid_scores)
        }
        
        return analysis
    
    def _calculate_relative_improvements(self, fid_scores):
        """ç›¸å¯¾çš„æ”¹å–„ç‡è¨ˆç®—"""
        baseline_score = fid_scores.get('mse', max(fid_scores.values()))
        
        improvements = {}
        for metric, score in fid_scores.items():
            if metric != 'mse':
                improvement = ((baseline_score - score) / baseline_score) * 100
                improvements[metric] = improvement
        
        return improvements
    
    def generate_visualization(self, analysis_results, output_path):
        """çµæœå¯è¦–åŒ–ã®ç”Ÿæˆ"""
        
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # FIDã‚¹ã‚³ã‚¢æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•
        metrics = list(analysis_results['fid_scores'].keys())
        scores = list(analysis_results['fid_scores'].values())
        
        plt.figure(figsize=(12, 8))
        bars = plt.bar(metrics, scores, color=sns.color_palette("husl", len(metrics)))
        plt.title('FID Score Comparison Across Optimization Metrics', fontsize=16)
        plt.ylabel('FID Score (Lower is Better)', fontsize=12)
        plt.xlabel('Optimization Metric', fontsize=12)
        
        # æ•°å€¤è¡¨ç¤º
        for bar, score in zip(bars, scores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    f'{score:.1f}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(output_path / 'fid_comparison_chart.png', dpi=300)
        plt.close()
```

## ğŸš€ å®Ÿè¡Œå¯èƒ½ãªå®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«

### experiments/fid_comparison/pilot_experiment.py
```python
#!/usr/bin/env python3
"""FIDæœ€é©åŒ–å®Ÿé¨“: ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆç‰ˆï¼ˆå°è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼‰"""

import torch
import time
from pathlib import Path
from generative_latent_optimization import OptimizationConfig
from generative_latent_optimization.workflows import process_bsds500_dataset
from generative_latent_optimization.metrics import DatasetFIDEvaluator

def run_pilot_experiment(max_images=10):
    """ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“å®Ÿè¡Œ"""
    
    # åŸºæœ¬è¨­å®š
    bsds500_path = os.environ.get('BSDS500_PATH')
    output_base = Path('./experiments/fid_comparison/results/pilot')
    
    # å®Ÿé¨“å¯¾è±¡æŒ‡æ¨™ï¼ˆãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã§ã¯åŸºæœ¬æŒ‡æ¨™ã®ã¿ï¼‰
    loss_functions = ['mse', 'l1']  # 'lpips', 'ssim'ã¯å®Ÿè£…å¾Œè¿½åŠ 
    
    results = {}
    
    for loss_func in loss_functions:
        print(f"ğŸ”„ Pilot experiment: {loss_func} optimization")
        
        config = OptimizationConfig(
            iterations=50,  # ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã§ã¯çŸ­ç¸®
            learning_rate=0.4,
            loss_function=loss_func
        )
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        start_time = time.time()
        datasets = process_bsds500_dataset(
            bsds500_path,
            output_base / f"{loss_func}_dataset",
            config,
            max_images_per_split=max_images,
            create_png_dataset=True
        )
        processing_time = time.time() - start_time
        
        # FIDè©•ä¾¡
        fid_evaluator = DatasetFIDEvaluator()
        fid_result = fid_evaluator.evaluate_created_dataset_vs_original(
            datasets['png'], bsds500_path
        )
        
        results[loss_func] = {
            'fid_score': fid_result.fid_score,
            'processing_time': processing_time,
            'total_images': fid_result.total_images
        }
        
        print(f"âœ… {loss_func}: FID = {fid_result.fid_score:.2f} ({processing_time/60:.1f}min)")
    
    return results

if __name__ == "__main__":
    results = run_pilot_experiment()
    print("\nğŸ“Š Pilot Experiment Results:")
    for metric, result in results.items():
        print(f"  {metric}: FID = {result['fid_score']:.2f}")
```

## ğŸ§® æ¨å®šãƒªã‚½ãƒ¼ã‚¹è¦ä»¶

### è¨ˆç®—è³‡æº
- **GPU**: NVIDIA RTX 3080/4080ä»¥ä¸Šæ¨å¥¨
- **VRAM**: æœ€ä½12GBï¼ˆLPIPSä½¿ç”¨æ™‚ã¯16GBæ¨å¥¨ï¼‰
- **RAM**: 32GBä»¥ä¸Šï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 100GBä»¥ä¸Šï¼ˆå®Ÿé¨“çµæœä¿å­˜ç”¨ï¼‰

### å‡¦ç†æ™‚é–“æ¨å®š

#### ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆå®Ÿé¨“ï¼ˆ30æšï¼‰
- MSE/L1: å„ç´„20åˆ†
- LPIPS: å„ç´„40åˆ†ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ï¼‰
- SSIMç³»: å„ç´„30åˆ†
- **åˆè¨ˆ**: ç´„3æ™‚é–“

#### å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿé¨“ï¼ˆ500æšï¼‰
- MSE/L1: å„ç´„6æ™‚é–“
- LPIPS: å„ç´„12æ™‚é–“
- SSIMç³»: å„ç´„8æ™‚é–“
- **åˆè¨ˆ**: ç´„48æ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œã§24æ™‚é–“ã«çŸ­ç¸®å¯èƒ½ï¼‰

### ä¸¦åˆ—å‡¦ç†æˆ¦ç•¥
```python
# 2GPUç’°å¢ƒã§ã®æœ€é©å‰²ã‚Šå½“ã¦
gpu_assignments = {
    'cuda:0': ['mse', 'l1', 'psnr'],        # é«˜é€ŸæŒ‡æ¨™
    'cuda:1': ['lpips', 'ssim', 'improved_ssim']  # é‡ã„æŒ‡æ¨™
}
```

## âœ… å“è³ªä¿è¨¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å®Ÿè£…å“è³ª
- [ ] ã™ã¹ã¦ã®æå¤±é–¢æ•°ã§ã®å‹¾é…è¨ˆç®—ç¢ºèª
- [ ] ãƒãƒƒãƒå‡¦ç†ã§ã®çµæœä¸€è²«æ€§ç¢ºèª
- [ ] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢ç¢ºèª
- [ ] æ•°å€¤å®‰å®šæ€§ã®æ¥µå€¤ãƒ†ã‚¹ãƒˆ
- [ ] ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ç¶²ç¾…çš„ãƒ†ã‚¹ãƒˆ

### å®Ÿé¨“å“è³ª
- [ ] åŒä¸€æ¡ä»¶ã§ã®å†ç¾æ€§ç¢ºèªï¼ˆÂ±1% FIDã‚¹ã‚³ã‚¢ï¼‰
- [ ] FIDè¨ˆç®—ã®å¦¥å½“æ€§ç¢ºèªï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ â‰ˆ 0ï¼‰
- [ ] çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç¢ºä¿ï¼ˆnâ‰¥30ï¼‰
- [ ] å¤–ã‚Œå€¤å‡¦ç†ã®é©åˆ‡æ€§ç¢ºèª

### çµæœå“è³ª
- [ ] FIDã‚¹ã‚³ã‚¢ã®å¦¥å½“ãªç¯„å›²ç¢ºèªï¼ˆ0-200ç¨‹åº¦ï¼‰
- [ ] æŒ‡æ¨™é–“ã®è«–ç†çš„æ•´åˆæ€§ç¢ºèª
- [ ] äºˆæƒ³çµæœã¨ã®æ•´åˆæ€§ç¢ºèª
- [ ] çµ±è¨ˆæ¤œå®šã«ã‚ˆã‚‹æœ‰æ„å·®ç¢ºèª

---

ã“ã®æ‹¡å¼µè¨­è¨ˆã«ã‚ˆã‚Šã€BSDS500ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®åŒ…æ‹¬çš„ãªFIDæœ€é©åŒ–å®Ÿé¨“ãŒå®Ÿç¾å¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚å®Ÿè£…ã¯æ®µéšçš„ã«é€²ã‚ã€å„æ®µéšã§ã®å“è³ªç¢ºèªã‚’å¾¹åº•ã™ã‚‹ã“ã¨ã§ã€ä¿¡é ¼æ€§ã®é«˜ã„å®Ÿé¨“çµæœã‚’ç²å¾—ã§ãã¾ã™ã€‚